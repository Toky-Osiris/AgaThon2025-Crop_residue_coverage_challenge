{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data into COCO Json file format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pycocotools import mask as maskUtils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "masks_dir = r'C:\\Users\\t.andriamihajasoa\\AgaThon2025-Crop_residue_coverage_challenge\\AgaThon2025-Crop_residue_coverage_challenge\\data\\raw\\masks'\n",
    "original_dir = r'C:\\Users\\t.andriamihajasoa\\AgaThon2025-Crop_residue_coverage_challenge\\AgaThon2025-Crop_residue_coverage_challenge\\data\\raw\\original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coco_annotation(image_id, image_filename, mask_dir):\n",
    "    annotations = []\n",
    "    mask_path = os.path.join(mask_dir, image_filename.replace('.jpg', '.tif'))\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        return annotations\n",
    "\n",
    "    # Apply connected components analysis\n",
    "    num_labels, labels_im = cv2.connectedComponents(mask)\n",
    "\n",
    "    for label in range(1, num_labels):  # Start from 1 to skip the background\n",
    "        # Create a binary mask for the current label\n",
    "        component_mask = (labels_im == label).astype(np.uint8) * 255\n",
    "\n",
    "        # Find the bounding box of the connected component\n",
    "        x, y, w, h = cv2.boundingRect(component_mask)\n",
    "        bbox = [x, y, w, h]\n",
    "\n",
    "        # Encode the mask to RLE\n",
    "        rle = maskUtils.encode(np.asfortranarray(component_mask))\n",
    "        area = maskUtils.area(rle)\n",
    "\n",
    "        # Find the segmentation\n",
    "        segmentation = np.transpose(np.nonzero(component_mask)).flatten().tolist()\n",
    "\n",
    "        annotation = {\n",
    "            \"id\": len(annotations) + 1,\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": 1,\n",
    "            \"segmentation\": [segmentation],\n",
    "            \"area\": area.tolist(),\n",
    "            \"bbox\": bbox,\n",
    "            \"iscrowd\": 0\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "    return annotations\n",
    "\n",
    "def create_coco_json(image_dir, mask_dir, output_json):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    image_id = 1\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    for image_filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Skipping {image_filename} as it could not be read.\")\n",
    "            continue\n",
    "\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        images.append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": image_filename,\n",
    "            \"height\": height,\n",
    "            \"width\": width\n",
    "        })\n",
    "\n",
    "        image_annotations = create_coco_annotation(image_id, image_filename, mask_dir)\n",
    "        annotations.extend(image_annotations)\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    coco_json = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": [{\"id\": 1, \"name\": \"object\"}]\n",
    "    }\n",
    "\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(coco_json, f, indent=4)\n",
    "\n",
    "def display_image_with_annotations(image_path, annotations):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read {image_path}\")\n",
    "        return\n",
    "\n",
    "    for annotation in annotations:\n",
    "        bbox = annotation['bbox']\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        for segmentation in annotation['segmentation']:\n",
    "            points = np.array(segmentation).reshape((-1, 2)).astype(np.int32)\n",
    "            cv2.polylines(image, [points], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    cv2.imshow('Image with Annotations', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 568/568 [05:38<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create COCO JSON annotations for our dataset\n",
    "create_coco_json(original_dir, masks_dir, r'C:\\Users\\t.andriamihajasoa\\AgaThon2025-Crop_residue_coverage_challenge\\AgaThon2025-Crop_residue_coverage_challenge\\data\\raw\\instances.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\t.andriamihajasoa\\AgaThon2025-Crop_residue_coverage_challenge\\AgaThon2025-Crop_residue_coverage_challenge\\data\\processed\\instances.json', 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Display the first image with annotations\n",
    "first_image_info = coco_data['images'][0]\n",
    "first_image_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == first_image_info['id']]\n",
    "display_image_with_annotations(os.path.join(r'C:\\Users\\t.andriamihajasoa\\AgaThon2025-Crop_residue_coverage_challenge\\AgaThon2025-Crop_residue_coverage_challenge\\data\\processed\\images', first_image_info['file_name']), first_image_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
